---
title: "P8105_HW6_jl7035"
author: "Jeffrey Lin"
date: "2024-12-02"
output: html_document
---

# Load Libraries
```{r}
library(tidyverse)
library(modelr)
library(mgcv)

```

# Problem 1 

## Load and Clean Data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```
## Peform Boostrap
```{r}
bootstrap_df <- weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, \(samp) lm(tmax ~ tmin, data = samp)),
    results = map(models, broom::tidy),
    summary = map(models, broom::glance),
    r_squared = map_dbl(summary, \(x) x$r.squared),
    log_beta_prod = map_dbl(results, \(x) log(prod(x$estimate)))
  ) %>% 
  select(-models,-results, -summary, -strap)
```

## Distribution of R-squared

```{r}
bootstrap_df %>% 
  ggplot(aes(x = r_squared)) +
  geom_density() + 
  xlab(label = "R-Squared") +
  ggtitle(label = "Distribution of R-Squared Values")

```
Examining the distribution of R-squared values, it appears that the R-squared 
for a linear model with minimum temperature as a predictor of max temperature 
tends to center at a little above 0.91. 

## Distribution of log(beta_1 * beta_0)

```{r}
bootstrap_df %>% 
  ggplot(aes(x = log_beta_prod)) + 
  geom_density() +
  xlab(label = "Log(Beta_0 * Beta_1") +
  ggtitle(label = "Distribution of Log(Beta_0 * Beta_1)")
```
Examining the distribution of Log(Beta_0 * Beta_1) values, it appears that the
values for a linear model with minimum temperature as a predictor of max temperature 
tends to center between 2.00 and 2.025. 

## Identify 95% confidence Interval for R-Squared

```{r}
bootstrap_df %>% 
  summarize(
    lower_ci = quantile(r_squared, 0.025), 
    upper_ci = quantile(r_squared, 0.975)
  )
```
## Identify the 95% confidence interval of Log (Beta_0 * Beta_1) 

```{r}
bootstrap_df %>% 
  summarize(
    lower_ci = quantile(log_beta_prod, 0.025), 
    upper_ci = quantile(log_beta_prod, 0.975)
  )
```

# Problem 2 

## Load in Data
```{r}
homicide_df <- read_csv(
  file = "Data/homicide-data.csv",
  na = c("NA", "",".", "na")) %>%
  drop_na() %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    victim_age = as.numeric(victim_age),
    victim_race = as_factor(victim_race),
    solve_status = factor(case_when(
      disposition %in% c("Closed without arrest", "Open/No arrest") ~ 0,
      disposition == "Closed by arrest" ~ 1,),
      labels = c("Unresolved", "Resolved")
    )
  ) %>% 
  filter(
    !(city_state %in% 
        c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")
      ),
    victim_race %in% c("Black", "White"),
    victim_sex %in% c("Female", "Male")
  ) 

```
## Prediction of Unresolved vs Resolved in Baltimore 
```{r}
balt_log_reg <- homicide_df %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(solve_status ~ victim_age + victim_sex + victim_race,
      data = .,
      family = binomial()
  )

balt_log_reg %>%
  broom::tidy(conf.int = TRUE) %>%
  filter(term == "victim_sexMale") %>% 
  mutate(
    adjusted_OR = exp(estimate),
    adjusted_OR_ci_lower = exp(conf.low),
    adjusted_OR_ci_upper = exp(conf.high)
  ) %>% 
  select(adjusted_OR:adjusted_OR_ci_upper, p.value)
```
## Logistic Regression for each City
```{r}
log_model_city <- homicide_df %>% 
  group_by(city_state) %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(data, 
        \(x) glm(x$solve_status ~ x$victim_age + x$victim_sex + x$victim_race,
        data = .,
        family = binomial())),
    summary = map(models, \(x) broom::tidy(x, conf.int = TRUE))
  ) %>% 
  unnest(summary) %>% 
  filter(term == "x$victim_sexMale") %>% 
  mutate(
    adjusted_OR = exp(estimate),
    adjusted_OR_ci_lower = exp(conf.low),
    adjusted_OR_ci_upper = exp(conf.high),
  ) %>% 
  select(city_state, adjusted_OR:adjusted_OR_ci_upper, p.value)
```

# OR and CI of Solved Crime Rates of Men compared to Women by City
```{r}
log_model_city %>%  
  ggplot(aes(x = reorder(city_state, adjusted_OR), y = adjusted_OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = adjusted_OR_ci_lower, ymax = adjusted_OR_ci_upper)) +
  xlab(label = "City-State") +
  ylab(label = "Adjusted Odds Ratio") +
  ggtitle("Odds Ratio of Crimes being Solved of Men Compared to Women by
          City") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```
We see the lowest adjusted odds ratio in New York City, where male victims of 
crimes are less likely to have their crimes resolved. Inversely, the highest 
odds ratio is found Albuquerque, where male victims of crimes are more likely to
have their crimes resolved. It is also interesting to note that the cities with 
the highest adjusted odds ratios also had the largest confidence intervals. 
These large confidence intervals cross the threshold of 1, so it we cannot be 
certain at a 95% confidence level that male victims are more likely to have 
their crimes resolved. Also broadly, it appears that the confidence intervals 
are increasing as the odds ratio increases.

# Problem 3

## Load and Clean Data
```{r}
birthweight_df <- 
  read_csv(
    file = "Data/birthweight.csv",
    na = c("NA", "na", ".", "")
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, levels = c(1,2), labels = c("male", "female")),
    frace = factor(
      frace, levels = c(1, 2, 3, 4, 8, 9), 
      labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(
      malform, levels = c(0, 1), labels = c("absent", "present")),
    mrace = factor(
      mrace, levels = c(1, 2, 3, 4, 8), 
      labels = c("White", "Black", "Asian", "Puerto Rican", "Other")
      )
  ) %>% drop_na()

```

## Rationale for Model 

Considering that it is difficult to accurately hypothesize which of these 
predictor variables are statistically significantly and strongly associated 
with the outcome variable, without further background knowledge or further 
exploratory data analysis, I propose to take an automatic variable selection 
procedure. In light of the difficulty with interpreting beta coefficients 
produced by lasso, as well as the relatively few number of predictor variables 
in this data set, I propose the use of a step wise regression model. Pertaining 
to the step wise regression, I choose to use backwards selection to account for 
possible order effects that would come into play from forward selection.

## Build Model
```{r}
model_all_vars <- lm(bwt ~ . , data = birthweight_df)
stepwise_model <- step(model_all_vars, direction = "backward")
summary(stepwise_model)
```

## Function for Stepwise Regression

```{r}
build_stepwise = function(df) {
  model_all_vars <- lm(bwt ~ . , data = df)
  stepwise_model <- step(model_all_vars, direction = "backward")
  
  return(stepwise_model)
}

```

## Plot Model Predictions against Real Data

```{r}
birthweight_df %>% 
  modelr::add_residuals(stepwise_model) %>% 
  modelr::add_predictions(stepwise_model) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dotted") +
  xlab(label = "Fitted values") +
  ylab(label = "Residuals") +
  ggtitle(label = "Residuals vs Fited")
```
Although the values are tend cluster densely between 2000 and 4000, they appear 
to be roughly random in location around the residual line. That is to say, there
does not appear to be a pattern in the residuals and they cluster roughly 
horizontally. 

## Length at Birth and Gestational Age Model

```{r}
blength_ga_model <- lm(bwt ~ blength + gaweeks, data = birthweight_df)
summary(blength_ga_model)

```


## head circumference, length, sex model

```{r}
bhead_blength_sex_model <- lm(bwt ~ bhead*blength*babysex, data = birthweight_df)
summary(bhead_blength_sex_model)
```

## Cross Validate each Model

```{r results = 'hide'}
cv_df <- 
  modelr::crossv_mc(birthweight_df, 100) %>% 
  mutate(
    stepwise_mod = map(train, \(df) build_stepwise(df)),
    blength_ga_mod = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    head_length_sex_mod = map(train, \(df) lm(bwt ~ bhead*blength*babysex, 
                                              data = df))
  ) %>% 
  mutate(
    rmse_stepwise = map2_dbl(
      stepwise_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_blength_ga = map2_dbl(
      blength_ga_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_head_len_sex_mod = map2_dbl(
      head_length_sex_mod, test, \(mod, df) rmse(model = mod, data = df)
    )
  )
```

# Plot RMSE by model
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    names_prefix = "rmse_",
    values_to = "rmse"
  ) %>% 
  mutate(
    model = fct_inorder(model)
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  xlab("Model") +
  ylab("RMSE") +
  ggtitle("RMSE by Model for Predicting Birthweight")
```
Comparing the 3 models, we can see that the stepwise model outperforms both the 
birth length and gestational model, and the head circumference, birth length, 
and baby sex models. The median RMSE of the stepwise model appears to be a 
bit below 275, which is less than the RMSE of the head_len_sex model (~285) and
the blength_ga model (~325).
